---
title: "Data Preparation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Data Preparation}
  \usepackage[utf8]{inputenc}
---

Data Preparation
====

## Part A) Profiling Data

**Overview**: Quantity of zeros, NA, Inf, unique values; as well as the data type may lead to a good or bad model. Here an approach to cover the very first step in data modeling. 

```{r lib, results="hide"}
## Loading funModeling !
#suppressMessages(library(funModeling))
#data(heart_disease)
```
KJLKJSH.KJASD
### Checking NA, zeros, data type and unique values
```{r df_status}
my_data_status=df_status(heart_disease)
```
* `q_zeros`: quantity of zeros (`p_zeros`: in percentage)
* `q_inf`:  quantity of infinite values (`p_na`: in percentage)
* `q_na`:  quantity of NA (`p_na`: in percentage)
* `type`: factor or numeric
* `unique`: quantity of unique values

#### Why are these metrics important?
* **Zeros**: Variables with **lots of zeros** may be not useful for modeling, and in some cases it may dramatically bias the model.
* **NA**: Several models automatically exclude rows with NA (**random forest**, for example). As a result, the final model can be biased due to several missing rows because of only one variable. For example, if the data contains only one out of 100 variables with 90% of NAs, the model will be training with only 10% of original rows.
* **Inf**: Infinite values may lead to an unexpected behavior in some functions in R.
* **Type**: Some variables are encoded as numbers, but they are codes or categories, and the models **don't handle them** in the same way.
* **Unique**: Factor/categorical variables with a high number of different values (~30), tend to do overfitting if categories have low representative, (**decision tree**, for example).

#### Filtering unwanted cases
Function `df_status` takes a data frame and returns a the status table to quickly remove unwanted cases.


**Removing variables with high number of NA/zeros**
```{r df_status3}
# Removing variables with 60% of zero values
vars_to_remove=subset(my_data_status, my_data_status$p_zeros > 60)
vars_to_remove["variable"]

## Keeping all except vars_to_remove 
heart_disease_2=heart_disease[, !(names(heart_disease) %in% vars_to_remove[,"variable"])]

```

**Ordering data by percentage of zeros**
```{r df_status4}
my_data_status[order(-my_data_status$p_zeros), c('variable', 'p_zeros')] 
```

<br>

-----------------------

## Part B) Treatment of Outliers

**Overview**: `prep_outliers` function tries to automatize as much as it can be outliers preparation. It focus on the values that influence heavily the mean.
It sets an `NA` or stop at a certain value all outliers for the desired variables.
<br>

**Outlier threshold**: The method to detect them is based on percentile, flagging as outlier if the value is on the top X % (commonly 0.5%, 1%, 2%). Setting parameter `top_percent` in `0.01` will flag all values on the top 1%.

<br>

Same logic goes for the lowest values, setting parameter `bottom_percent` in 0.01 will flag as an outlier the lowest 1% of all values.

**Models highly affected by a biased mean**: linear regression, logistic regression, kmeans, decision trees. Random forest deals better with outliers. 
 
**Automatization**: `prep_outliers` skip all factor/char columns, so it can receive a whole data frame, removing outliers by finally, returning a the _cleaned_ data.

<br>
 
This function covers two typical scenarios (parameter `type`):

* Case 1: Descriptive statistics / data profiling
* Case 2: Data for predictive model


### Case 1: `type='set_na'`

In this case all outliers are converted into `NA`, thus applying most of the descriptive functions (max, min, mean) will return a **less-biased mean** value - with the proper `na.rm=TRUE` parameter.


### Case 2: `type='stop'`

Last case will cause that all rows with `NA` values will lost when a machine learning model is created. To avoid this, but keep controlled the outliers, all values flagged as outlier will be converted to the threshold value.

**Key notes**: 

* Try to think variables treatment (and creation) as if you're explaining to the model. Stopping variables at a certain value, 1% for example, you are telling to the model: _consider all extremes values as if they are on the 99% percentile, this value is already high enough_
* Models try to be noise tolerant, but you can help them by treat some common issues.


## Examples

```{r outliers_treatment1,  fig.height=3, fig.width=4}
########################################
# Creating data frame with outliers
########################################
set.seed(10)
df=data.frame(var1=rchisq(1000,df = 1), var2=rnorm(1000))
df=rbind(df, 1135, 2432) # forcing outliers
df$id=as.character(seq(1:1002))

# for var1: mean is ~ 4.56, and max 2432
summary(df)


```

### Case 1: `type='set_na'`

```{r outliers_treatment2,  fig.height=3, fig.width=4}
########################################################
### CASE 1: Treatment outliers for data profiling
########################################################

#### EXAMPLE 1: Removing top 1% for a single variable

# checking the value for the top 1% of highest values (percentile 0.99), which is ~ 7.05
quantile(df$var1, 0.99)

# Setting type='set_na' sets NA to the highest value)
var1_treated=prep_outliers(data = df,  str_input = 'var1',  type='set_na', top_percent  = 0.01)

# now the mean (~ 0.94) is less biased, and note that: 1st, median and 3rd quartiles remaining very similar to the original variable.
summary(var1_treated)

#### EXAMPLE  2: if 'str_input' is missing, then it runs for all numeric variables (which have 3 or more distinct values).
df_treated2=prep_outliers(data = df, type='set_na', top_percent  = 0.01)
summary(df_treated2)

#### EXAMPLE  3: Removing top 1% (and bottom 1%) for 'N' specific variables.
vars_to_process=c('var1', 'var2')
df_treated3=prep_outliers(data = df, str_input = vars_to_process, type='set_na', bottom_percent = 0.01, top_percent  = 0.01)
summary(df_treated3)

```

### Case 2: `type='stop'`

```{r outliers_treatment3,  fig.height=3, fig.width=4}
########################################################
### CASE 2: Treatment outliers for predictive modeling
########################################################
#### EXAMPLE 4: Stopping outliers at the top 1% value for all variables. For example if the top 1% has a value of 7, then all values above will be set to 7. Useful when modeling because outlier cases can be used.
df_treated4=prep_outliers(data = df, type='stop', top_percent = 0.01)

# before
summary(df$var1)

# after, the max value is 7
summary(df_treated4$var1)

 	
```

### Plots
Note when `type='set_na'` last points disappear
```{r outliers_treatment4,  fig.height=3, fig.width=4}
ggplot(df_treated3, aes(x=var1)) + geom_histogram(binwidth=.5) + ggtitle("Setting type='set_na' (var1)")
ggplot(df_treated4, aes(x=var1)) + geom_histogram(binwidth=.5) + ggtitle("Setting type='stop' (var1)")
```


<br>

-----------------------

## Part C) Transforming variable into [0-1] range

Transforming a single numeric vector into 0 to 1 range. 

```{r}
age01=range01(heart_disease$age)
summary(age01)
```

<br>
